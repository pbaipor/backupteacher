{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Database\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "pd.options.display.max_rows = 10\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "**The main objective of the PCA is to analyze the data to identify patterns in order to reduce the dimensionality of the data with a minimum loss of information**. One possible application would be pattern recognition, where we want to reduce computational costs and parameter estimation error by reducing the dimensionality of our attribute space by extracting a subspace that describes our data \"satisfactorily.\" **The dimensionality reduction becomes important when we have a number of attributes significantly larger than the number of training samples**.\n",
    "\n",
    "We apply the PCA to design all of our data (without class labels) in a different subspace, looking for the axes with the maximum variance where the data is most distributed. The main question is: ** \"What subspace does our data represent?\" **.\n",
    "\n",
    "** First, we calculate the eigenvectors (major components) of our data and organize them into a projection matrix. Each eigenvector * is associated with an eigenvalue * which can be interpreted as the \"size\" or \"magnitude\" of the corresponding eigenvector **. In general, we consider only the eigenvalues that have a significantly larger magnitude than the others and disregard the autopares (eigenvector eigenvalues) that we consider to be less informative.\n",
    "\n",
    "If we observe that all eigenvalues have a similar magnitude, this may be a good indicator that our data is already in a * good * subspace. On the other hand, ** if some eigenvalues have a much greater magnitude than the others, we must choose their eigenvectors since they contain more information about the distribution of our data **. Likewise, eigenvalues close to zero are less informative and we should disregard them in building our subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA vs LDA\n",
    "Both PCA and LDA (*Linear Discrimant Analysis*) are linear transformation methods. On the one hand, the PCA provides the directions (eigenvectors or major components) that maximizes the variance of the data, while the LDA targets the directions that maximize the separation (or discrimination) between different classes. Maximizing the variance in the case of PCA also means reducing the loss of information, which is represented by the sum of the projection distances of the data in the axes of the main components.\n",
    "\n",
    "While the PCA designs the data in a different subspace, the LDA attempts to determine a suitable subspace to distinguish the patterns belonging to the different classes.\n",
    "\n",
    "<img src=\"images/PCAvsLDA.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing the PCA approach in 4 steps\n",
    "In general, the application of PCA involves the following steps:\n",
    "    \n",
    "1. Standardization of data\n",
    "2. Obtaining eigenvectors and eigenvalues through:\n",
    "     - Covariance Matrix; or\n",
    "     - Matrix of Correlation; or\n",
    "     - *Singular Vector Decomposition*\n",
    "3. Construction of the projection matrix W from the selected eigenvectors\n",
    "4. Transformation of the original data X via W to obtain the subspace Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following tutorial, we will be working with the famous “Iris” dataset that has been deposited on the UCI machine learning repository <a href = \"https://archive.ics.uci.edu/ml/datasets/Iris\"> (Iris Dataset)</a>\n",
    ".\n",
    "\n",
    "**Reference:** Bache, K. & Lichman, M. (2013). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science.\n",
    "The iris dataset contains measurements for 150 iris flowers from three different species.\n",
    "\n",
    "The three classes in the Iris dataset:\n",
    "\n",
    "1. Iris-setosa (n=50)\n",
    "2. Iris-versicolor (n=50)\n",
    "3. Iris-virginica (n=50)\n",
    "\n",
    "The four features of the Iris dataset:\n",
    "\n",
    "1. sepal length in cm\n",
    "2. sepal width in cm\n",
    "3. petal length in cm\n",
    "4. petal width in cm\n",
    "\n",
    "<img src=\"images/Iris-dataset.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "## Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "print(iris.data.shape)\n",
    "print(iris.target.shape)\n",
    "print(iris.feature_names)\n",
    "print(iris.target_names)\n",
    "\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "df['class'] = iris.target\n",
    "df['class'] = df['class'].map({0:iris.target_names[0], 1:iris.target_names[1], 2:iris.target_names[2]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data table into data X and class labels Y\n",
    "x = df.iloc[:, 0:4].values\n",
    "y = df.iloc[:, 4].values\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "for f in range(4):\n",
    "    plt.subplot(2, 2, f+1)\n",
    "    for label in iris.target_names:\n",
    "        plt.hist(x[y==label, f], label=label, bins=10, alpha=0.3)\n",
    "        plt.xlabel(iris.feature_names[f])\n",
    "plt.legend(loc='upper right', fancybox=True, fontsize=8)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data standardization\n",
    "First, we will normalize the data to the normal distribution (mean = 0, standard deviation = 1) using the following formula (where each attribute is normalized individually):\n",
    "\n",
    "$$std\\_unit = \\frac{x-\\mu}{\\sigma}$$\n",
    "\n",
    "which can be used through the * StandardScaler * module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "x_std = StandardScaler().fit_transform(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "### Eigenvectors and eigenvalues\n",
    "The eigenvectors and eigenvalues of a covariance (or correlation) matrix represent the basis of the PCA: eigenvectors (main components) determine the direction of the new attribute space, and eigenvalues determine its magnitude. In other words, the eigenvalues explain the variance of the data along the new attribute axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance Matrix\n",
    "The classical PCA approach calculates the covariance matrix, where each element represents the covariance between two attributes. The covariance between two attributes is calculated as follows:\n",
    "\n",
    "$$\\sigma_{jk} = \\frac{1}{n-1}\\sum_{i=1}^N(x_{ij}-\\overline{x}_j)(x_{ik}-\\overline{x}_k)$$\n",
    "\n",
    "That we can simplify in the vector form through the formula:\n",
    "\n",
    "$$S=\\frac{1}{n-1}((x-\\overline{x})^T(x-\\overline{x}))$$\n",
    "\n",
    "where $ \\overline{x}$ is a d-dimensional vector where each value represents the mean of each attribute, and $ n $ represents the number of attributes per sample. It is worth mentioning that x is a vector where each sample is organized in rows and each column represents an attribute. If you have a vector where the samples are organized into columns and each row represents an attribute, the transpose passes to the second element of the multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vec = np.mean(x_std, axis=0)\n",
    "cov_mat = (x_std - mean_vec).T.dot((x_std - mean_vec)) / (x_std.shape[0] - 1)\n",
    "print(cov_mat, np.cov(x_std.T), sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, the result of the covariance matrix basically represents the following structure:\n",
    "\n",
    "$$\\begin{bmatrix}var(1) & cov(1,2) & cov(1,3) & cov(1,4) \n",
    "\\\\ cov(1,2) & var(2) & cov(2,3) & cov(2,4)\n",
    "\\\\ cov(1,3) & cov(2,3) & var(3) & cov(3,4)\n",
    "\\\\ cov(1,4) & cov(2,4) & cov(3,4) & var(4)\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "285/5000\n",
    "Where the main diagonal represents the variance in each dimension and the other elements are the covariance between each pair of dimensions.\n",
    "\n",
    "In order to calculate the eigenvalues and eigenvectors, we only need to call the function *np.linalg.eig*, where each eigenvector is represented by a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vals_cov, eig_vecs_cov = np.linalg.eig(cov_mat)\n",
    "print('eigenvectors', eig_vecs_cov, sep='\\n')\n",
    "print('eigenvalues', eig_vals_cov, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting property of the covariance matrix is that **the sum of the principal diagonal of the matrix (variance for each dimension) is equal to the sum of the eigenvalues**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(np.diagonal(cov_mat)), np.sum(eig_vals_cov))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix of Correlation\n",
    "Another way to calculate eigenvalues and eigenvectors is to use the correlation matrix. Although the matrices are different, they will result in the same eigenvalues and eigenvectors (shown later) since the correlation matrix is given by the normalization of the covariance matrix.\n",
    "\n",
    "$$corr(x,y) = \\frac{cov(x,y)}{\\sigma_x \\sigma_y}$$\n",
    "\n",
    "To show this, let us first compute the correlation matrices for normalized and non-normalized data for normal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_mat1 = np.corrcoef(x_std.T)\n",
    "cor_mat2 = np.corrcoef(x.T)\n",
    "print(cor_mat1, cor_mat2, sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vals_cor, eig_vecs_cor = np.linalg.eig(cor_mat1)\n",
    "print('eigenvectors', eig_vecs_cor, sep='\\n')\n",
    "print('eigenvalues', eig_vals_cor, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Vector Decomposition\n",
    "Although the autodecomposition (calculation of eigenvectors and eigenvalues) made by the covariance or correlation matrix is more intuitive, most PCA implementations perform *Singular Vector Decomposition* (SVD) to improve computational performance. To calculate the SVD, we can use the numpy library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vecs_svd,s,v = np.linalg.svd(x_std.T)\n",
    "print('eigenvectors', eig_vecs_svd, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that self-decoding results in the same eigenvalues and eigenvectors using any of the matrices below:\n",
    "\n",
    "- Covariance matrix after data standardization\n",
    "- Correlation matrix\n",
    "- Correlation matrix after data standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification of eigenvectors and eigenvalues\n",
    "In order to verify that the eigenvectors and eigenvalues calculated in the autodecomposition are correct, we must verify if they satisfy the equation for each eigenvector and corresponding eigenvalue:\n",
    "\n",
    "$$\\Sigma \\overrightarrow{v} = \\lambda \\overrightarrow{v}$$\n",
    "\n",
    "where:\n",
    "$$\\Sigma = Matrix\\,of\\,Covariance$$\n",
    "$$\\overrightarrow{v} = eigenvectors$$\n",
    "$$\\lambda = eigenvalues$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verification for covariance matrix\")\n",
    "for i in range(len(eig_vecs_cov[0])):\n",
    "    e_vec = eig_vecs_cov[:,i].reshape(1,4).T\n",
    "    print(cov_mat.dot(e_vec).T, (eig_vals_cov[i] * e_vec).T, sep='\\n', end='\\n\\n')\n",
    "    \n",
    "print(\"Verification for correlation matrix\")\n",
    "for i in range(len(eig_vecs_cor[0])):\n",
    "    e_vec = eig_vecs_cor[:,i].reshape(1,4).T\n",
    "    print(cor_mat1.dot(e_vec).T, (eig_vals_cor[i] * e_vec).T, sep='\\n', end='\\n\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of eigenvectors and eigenvalues\n",
    "As stated, the typical objective of PCA is to reduce the dimensionality of data by projection in a smaller subspace, where eigenvectors form the axes. However, the eigenvectors define only the directions of the new axes, since they all have size 1, which we can confirm by the following lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ev in eig_vecs_cov:\n",
    "    print(np.linalg.norm(ev), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, to decide which eigenvector (s) we can discard without losing much information in the construction of our subspace, we need to check the corresponding eigenvalues. ** The eigenvectors with the highest values are those that contain more information about the distribution of our data **. These are the eigenvectors we want.\n",
    "\n",
    "To do this, we must sort the eigenvalues in descending order to choose the top k eigenvectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_pairs = [(np.abs(eig_vals_cov[i]), eig_vecs_cov[:,i]) for i in range(len(eig_vals_cov))]\n",
    "eig_pairs.sort()\n",
    "eig_pairs.reverse()\n",
    "\n",
    "print(\"Eigenvalues in descending order and their respective eigenvectors:\")\n",
    "for ep in eig_pairs:\n",
    "    print(ep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Calculation\n",
    "After sorting the eigenvalues, the next step is to ** define how many major components will be chosen for our new subspace **. To do this, we can use the * explained variance * method, which calculates how much information (variance) is assigned to each major component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = sum(eig_vals_cov)\n",
    "var_exp = [(i / total)*100 for i in sorted(eig_vals_cov, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "for i in range(len(cum_var_exp)):\n",
    "    print(\"The highest eigenvalue (s) contains {1: .2f}% of information\".format(i+1, cum_var_exp[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, most of the variance (72.77%) can be explained by the largest main component alone, while the second contains 23.03% of information. **Together, the first two major components contain 95.80% of all information**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection Matrix\n",
    "In practice, the projection matrix is nothing more than the top k concatenated eigenvectors. So if we want to reduce our 4-dimensional space to a 2-dimensional space, we must choose the 2 eigenvectors with the 2 largest eigenvalues to construct our matrix W (d$\\times$k)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.hstack((eig_pairs[0][1].reshape(4,1), eig_pairs[1][1].reshape(4,1)))\n",
    "print(w) # [DxK]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection in the new subspace\n",
    "The last step of the PCA is to use our dimensional projection matrix W (4x2, where each column represents an eigenvector) to transform our samples into a new subspace. To do this, simply apply the following equation:\n",
    "\n",
    "$$S = (X-\\mu_X) \\times W$$\n",
    "\n",
    "Where each row in S contains the weights for each attribute (matrix column) in the new subspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = (x_std-mean_vec).dot(w)\n",
    "print(s.shape) # [NxK]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a matter of curiosity, note that if W represented all eigenvectors - not just the chosen ones - we could recompose each instance to X by the following formula:\n",
    "\n",
    "$$X = (S \\times W^{-1}) + \\mu_X$$\n",
    "\n",
    "Again, each line in S represents the weights for each attribute, only this time it would be possible to represent X by the sum of each eigenvector multiplied by a weight.\n",
    "\n",
    "In the example below, let's consider only the first instance of x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# projection of the first sample into x in the subspace represented by all eigenvectors\n",
    "x0 = x_std[0,:]\n",
    "w2 = eig_vecs_cov\n",
    "s2 = (x0-mean_vec).dot(w2)\n",
    "print(x0, w2, s2, sep='\\n', end='\\n\\n')\n",
    "\n",
    "# calculation of the first instance of x by subspace\n",
    "x0_back = s2.dot(np.linalg.inv(w2))+mean_vec\n",
    "print(x0_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showPCAReduction(s_matrix):\n",
    "    with plt.style.context('seaborn-whitegrid'):\n",
    "        plt.figure(figsize=(6,4))\n",
    "        for label, color in zip(iris.target_names, ['blue', 'red', 'green']):\n",
    "            plt.scatter(s_matrix[y==label, 0], s_matrix[y==label, 1], label=label, c=color)\n",
    "        plt.xlabel('Principal Component 1')\n",
    "        plt.ylabel('Principal Component 2')\n",
    "        plt.legend(loc='lower center')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "showPCAReduction(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA via scikit-learn\n",
    "We can compare our results using the implementation present in the scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA as sklPCA\n",
    "sklearn_pca = sklPCA(n_components=2)\n",
    "y_sklearn = sklearn_pca.fit_transform(x_std)\n",
    "print(y_sklearn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showPCAReduction(y_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General rules\n",
    "- ** Always ** normalize the attributes before applying the PCA (StandarScaler);\n",
    "- Remember to store the average for the round trip;\n",
    "- ** No ** apply the PCA after other attribute selection algorithms;\n",
    "- The number of major components you want to maintain should be chosen by analyzing the number of components and the accuracy of the system. ** Not always more main components result in better accuracy! **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
