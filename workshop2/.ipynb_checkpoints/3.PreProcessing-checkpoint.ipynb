{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing using scikit-learn\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/awantik/machine-learning-slides/blob/master/ML-Pipeline.png?raw=true\">\n",
    "## Agenda\n",
    "1. Introduction to Preprocessing\n",
    "2. StandardScaler\n",
    "3. MinMaxScaler\n",
    "4. RobustScaler\n",
    "5. Normalization\n",
    "6. Binarization\n",
    "7. Encoding Categorical (Ordinal & Nominal) Features\n",
    "8. Imputation\n",
    "9. Polynomial Features\n",
    "10. Custom Transformer\n",
    "11. Text Processing\n",
    "12. CountVectorizer\n",
    "13. Image using skimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to PreProcessing\n",
    "* Learning algorithms have affinity towards certain pattern of data.\n",
    "* Unscaled or unstandardized data have might have unacceptable prediction\n",
    "* Learning algorithms understands only number, converting text image to number is required\n",
    "* Preprocessing refers to transformation before feeding to machine learning\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/awantik/machine-learning-slides/blob/master/pp5.PNG?raw=true\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. StandardScaler\n",
    "* The StandardScaler assumes your data is normally distributed within each feature and will scale them such that the distribution is now centred around 0, with a standard deviation of 1.\n",
    "* Calculate - Subtract mean of column & div by standard deviation\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/awantik/machine-learning-slides/blob/master/pp4.PNG?raw=true\">\n",
    "\n",
    "\n",
    "* If data is not normally distributed, this is not the best scaler to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating normally distributed data\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'x1': np.random.normal(0, 2, 10000),\n",
    "    'x2': np.random.normal(5, 3, 10000),\n",
    "    'x3': np.random.normal(-5, 5, 10000)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting data\n",
    "\n",
    "df.plot.kde()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "standardscaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tf = standardscaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_tf, columns=['x1','x2','x3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.kde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MinMaxScaler\n",
    "* One of the most popular\n",
    "* Calculate - Subtract min of column & div by difference between max & min\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/awantik/machine-learning-slides/blob/master/pp3.PNG?raw=true\">\n",
    "\n",
    "\n",
    "\n",
    "* Data shifts between 0 & 1\n",
    "* If distribution not suitable for StandardScaler, this scaler works out.\n",
    "* Sensitive to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    # positive skew\n",
    "    'x1': np.random.chisquare(8, 1000),\n",
    "    # negative skew \n",
    "    'x2': np.random.beta(8, 2, 1000) * 40,\n",
    "    # no skew\n",
    "    'x3': np.random.normal(50, 3, 1000)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.kde()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax = MinMaxScaler()\n",
    "data_tf = minmax.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_tf,columns=['x1','x2','x3'])\n",
    "df.plot.kde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Robust Scaler\n",
    "* Suited for data with outliers\n",
    "* Calculate by subtracting 1st-quartile & div by difference between 3rd-quartile & 1st-quartile\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/awantik/machine-learning-slides/blob/master/pp2.PNG?raw=true\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    # Distribution with lower outliers\n",
    "    'x1': np.concatenate([np.random.normal(20, 1, 1000), np.random.normal(1, 1, 25)]),\n",
    "    # Distribution with higher outliers\n",
    "    'x2': np.concatenate([np.random.normal(30, 1, 1000), np.random.normal(50, 1, 25)]),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.kde()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "robustscaler = RobustScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tf = robustscaler.fit_transform(df)\n",
    "df = pd.DataFrame(data_tf, columns=['x1','x2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.kde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Normalizer\n",
    "* Each parameter value is obtained by dividing by magnitude\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/awantik/machine-learning-slides/blob/master/pp1.PNG?raw=true\">\n",
    "* Centralizes data to origin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'x1': np.random.randint(-100, 100, 1000).astype(float),\n",
    "    'y1': np.random.randint(-80, 80, 1000).astype(float),\n",
    "    'z1': np.random.randint(-150, 150, 1000).astype(float),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(df.x1, df.y1, df.z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "data_tf = normalizer.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_tf, columns=['x1','y1','z1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(df.x1, df.y1, df.z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Binarization\n",
    "* Thresholding numerical values to binary values ( 0 or 1 )\n",
    "* A few learning algorithms assume data to be in Bernoulli distribution - Bernoulli's Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[ 1., -1.,  2.],\n",
    "              [ 2.,  0.,  0.],\n",
    "              [ 0.,  1., -1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "binarizer = Binarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tf = binarizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Encoding Categorical Values \n",
    "#### Encoding Ordinal Values\n",
    "* Ordinal Values - Low, Medium & High. Relationship between values\n",
    "* LabelEncoding with right mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Age':[33,44,22,44,55,22],\n",
    "    'Income':['Low','Low','High','Medium','Medium','High']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Income.map({'Low':1,'Medium':2,'High':3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding Nominal Values\n",
    "* Nominal Values - Male, Female. No relationship between data\n",
    "* One Hot Encoding for converting data into one-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Age':[33,44,22,44,55,22],\n",
    "    'Gender':['Male','Female','Male','Female','Male','Male']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Gender.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df['gender_tf'] = le.fit_transform(df.Gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OneHotEncoder().fit_transform(df[['gender_tf']]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Imputation\n",
    "* Missing values cannot be processed by learning algorithms\n",
    "* Imputers can be used to infer value of missing data from existing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'A':[1,2,3,4,np.nan,7],\n",
    "    'B':[3,4,1,np.nan,4,5]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(strategy='mean', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Polynomial Features\n",
    "Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form $[a, b]$, the degree-2 polynomial features are $[1, a, b, a^2, ab, b^2].\n",
    "\n",
    "* Deriving non-linear feature by coverting data into higher degree\n",
    "* Used with linear regression to learn model of higher degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'A':[1,2,3,4,5], 'B':[2,3,4,5,6]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "pol = PolynomialFeatures(degree=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Custom Transformer \n",
    "* Often, you will want to convert an existing Python function into a transformer to assist in data cleaning or processing.\n",
    "* FunctionTransformer is used to create one Transformer \n",
    "* validate = False, is required for string columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping(x):\n",
    "    x['Age'] = x['Age']+2\n",
    "    x['Counter'] = x['Counter'] * 2\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customtransformer = FunctionTransformer(mapping, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Age':[33,44,22,44,55,22],\n",
    "    'Counter':[3,4,2,4,5,2],\n",
    "     })\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customtransformer.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Text Processing\n",
    "* Perhaps one of the most common information\n",
    "* Learning algorithms don't understand text but only numbers\n",
    "* Below menthods convert text to numbers\n",
    "\n",
    "## 12. CountVectorizer\n",
    "* Each column represents one word, count refers to frequency of the word\n",
    "* Sequence of words are not maintained\n",
    "\n",
    "#### Hyperparameters\n",
    "* n_grams - Number of words considered for each column\n",
    "* stop_words - words not considered\n",
    "* vocabulary - only words considered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "     'This is the first document awesome food.',\n",
    "     'This is the second second document.',\n",
    "     'And the third one the is mission impossible.',\n",
    "     'Is this the first document?',\n",
    "]\n",
    "df = pd.DataFrame({'Text':corpus})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.fit_transform(df.Text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words=['the','is'])\n",
    "cv.fit_transform(df.Text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(vocabulary=['mission','food','second'])\n",
    "cv.fit_transform(df.Text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=[1,2])\n",
    "cv.fit_transform(df.Text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Image Processing using skimage\n",
    "* skimage doesn't come with anaconda. install with 'pip install skimage'\n",
    "* Images should be converted from 0-255 scale to 0-1 scale.\n",
    "* skimage takes image path & returns numpy array\n",
    "* images consist of 3 dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread,imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = imread('images/ISIC_0000029.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb2gray(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(rgb2gray(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(resize(image, (200,200)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
