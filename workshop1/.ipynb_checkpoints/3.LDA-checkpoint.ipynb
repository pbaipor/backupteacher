{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "## Goals of this Tutorial\n",
    "\n",
    "- Understand \"What is Linear Discrimant Analysis\"\n",
    "- Using Python/Jupyter notebook and python libraries (e.g. panda, scikit-learn) for analysis features.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "***Linear Discriminant Analysis (LDA)*** is most commonly used as dimensionality reduction technique in the pre-processing step for pattern-classification and machine learning applications. The goal is to project a dataset onto a lower-dimensional space with good class-separability in order avoid overfitting (“curse of dimensionality”) and also reduce computational costs.\n",
    "\n",
    "The LDA approach is very similar to that of the PCA, however, **in addition to finding the components that maximize the variance of our data (PCA), we are also interested in components that maximize multiclass (LDA) separation**.\n",
    "\n",
    "In short, the objective of LDA is to design an attribute space (a database with n-dimensional samples) in a smaller subspace k (where $ k \\ leq n-1 $) while maintaining the discrimination information of the classes.\n",
    "\n",
    "In general, dimensionality reduction helps not only reduce computational costs for a given classification problem but is also useful to avoid overfitting by minimizing error in parameter estimation.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA vs LDA\n",
    "\n",
    "Both PCA and LDA are linear transformation techniques widely used for dimensionality reduction. On the one hand, PCA can be described as an \"unsupervised\" algorithm, since it \"ignores\" the class labels and its purpose is to find the directions (main components) that maximize the variance in the database. On the other hand, the LDA is \"supervised\" and calculates the directions (linear discriminants) that will represent the axes that maximize the separation between multiple classes.\n",
    "\n",
    "Although it seems that LDA is superior to PCA in multi-class problems where class labels are known, this is not always the case. For example, comparisons between classification accuracy for image recognition after PCA or LDA use shows **that PCA tends to be better than LDA if the number of samples / class is relatively small** (<a href=\"http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=908974\"> PCA vs. LDA </a>, AM Martinez et al., 2001). In practice, it is also common to use both LDA and PCA together, i.e., PCA for dimensionality reduction followed by an LDA.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/PCAvsLDA.png\" width=600>\n",
    "</p>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a “good” feature subspace?\n",
    "\n",
    "Assuming our goal is to reduce the dimensions of a *** d *** - dimensional dataset by projecting into a *** k *** - dimensional subspace (where *** k <d ***). So how do we know which size we should choose for *** k *** (*** k *** = the number of dimensions of the new attribute subspace), and how to know if we have a attribute space that represents \"well \" our data?\n",
    "\n",
    "Soon, we will calculate the eigenvectors (components) of our dataset and collect them in matrices called * scatter-matrices *, or rather, intra-class sparse matrices and inter-classes. Each of these eigenvectors is associated with an eigenvalue that tells us the \"size\" or \"magnitude\" of eigenvectors.\n",
    "\n",
    "** If we observe that all eigenvalues ​​have a similar magnitude, then this may be a good indicator that our data is already projected into a \"good\" attribute space. **\n",
    "\n",
    "On the other hand, if some eigenvalues ​​have a much greater magnitude than the others, we must choose their eigenvectors since they contain more information about the distribution of our data. Likewise, eigenvalues ​​close to zero are less informative and we should disregard them in building our subspace.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing the LDA approach in 5 steps\n",
    "\n",
    "Listed below are the 5 general steps for performing a linear discriminant analysis; we will explore them in more detail in the following sections.\n",
    "\n",
    "1. Calculate the mean (vector ** d ** - dimensional) for each of the classes in the dataset\n",
    "2. Calculate the scatter-matrices (intra-class and inter-class)\n",
    "3. Calculate the eigenvectors ($ e_1, e_2, ..., e_d $) and their corresponding eigenvalues ($ \\ lambda_1, \\ lambda_2, ... \\ lambda_d $) for the scatter-matrices.\n",
    "4. Sort the eigenvectors by the eigenvalues in descending order and choose the eigenvectors with the largest eigenvalues to form a matrix ** W ** [$ d \\ times k $], where each column represents an eigenvector.\n",
    "5. Use ** W ** to transform the samples into the new subspace. This can be summed up by multiplying matrices: $ Y = X \\ times W $ (where ** X ** [$ n \\ times d $] is the matrix representing our dataset with *** n *** samples, and ** Y ** is the matrix of the samples transformed into the new subspace [$ n \\ times k $]).\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# database\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "pd.options.display.max_rows = 10\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "## Importing dataset\n",
    "\n",
    "In this tutorial, we will use the ** Iris dataset **, already present in scikit-learn. The Iris dataset contains data of 150 flowers divided into 3 different species (setosa, versicolor, virginica). The data is:\n",
    "\n",
    "1. sepal lenght in cm\n",
    "2. sepal width in cm\n",
    "3. petal length in cm\n",
    "4. petal width in cm\n",
    "\n",
    "<img src=\"images/Iris-dataset.png\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "print(iris.data.shape)\n",
    "print(iris.target.shape)\n",
    "print(iris.feature_names)\n",
    "print(iris.target_names)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "df['class'] = iris.target\n",
    "# df['class'] = df['class'].map({0:iris.target_names[0], 1:iris.target_names[1], 2:iris.target_names[2]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data table into data X and class labels Y\n",
    "x = df.iloc[:, 0:4].values\n",
    "y = df.iloc[:, 4].values\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "for f in range(4):\n",
    "    plt.subplot(2, 2, f+1)\n",
    "    for label in iris.target_names:\n",
    "        plt.hist(x[y==(list(iris.target_names).index(label)), f], label=label, bins=10, alpha=0.3)\n",
    "        plt.xlabel(iris.feature_names[f])\n",
    "plt.legend(loc='upper right', fancybox=True, fontsize=8)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Just by looking at the graphs, it is easy to see that the attributes ** petal length ** and ** petal width ** are potential attributes to separate the flowers by the classes. In practice, instead of reducing dimensionality via projection (LDA), a good alternative would be to apply a feature selection technique.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normality assumptions\n",
    "\n",
    "Like other algorithms, the LDA assumes that the data are normally distributed, the attributes are statistically independent, and the covariance matrix is identical for each class. However, this only applies to LDA as a classifier. As a dimensionality reducer, the LDA also works reasonably well if these assumptions are violated. And even for classification tasks the LDA can be robust the data distribution:\n",
    "\n",
    "<a> \"Linear discriminant analysis frequently achieves good performances in the tasks of face and object recognition, even though the assumptions of common covariance matrix among groups and normality are often violated (Duda, et al., 2001)\" <a href = \"https://link.springer.com/article/10.1007/s10115-006-0013-y\">(Tao Li, et al., 2006) </a>.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discrimant Analysis in 5 steps\n",
    "\n",
    "## Step 1: Computing the d-dimensional mean vectors\n",
    "\n",
    "Let us begin by calculating the mean vectors ** $ m_i, i = (0,1,2) $ ** for the 3 classes of flowers:\n",
    "\n",
    "$$\n",
    "\\pmb m_i = \\begin{bmatrix}\n",
    "\\mu_{\\omega_i (\\text{sepal length)}}\\\\\n",
    "\\mu_{\\omega_i (\\text{sepal width})}\\\\\n",
    "\\mu_{\\omega_i (\\text{petal length)}}\\\\\n",
    "\\mu_{\\omega_i (\\text{petal width})}\\\\\n",
    "\\end{bmatrix} \\; , \\quad \\text{for} \\quad i = 0,1,2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vectors = []\n",
    "for c in range(0,3):\n",
    "    mean_vectors.append(np.mean(x[y==c], axis=0))\n",
    "    print('Mean vector class {0}: {1}'.format(c, mean_vectors[c]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## Step 2: Computing the Scatter Matrices\n",
    "\n",
    "Now let's calculate the two 4x4 matrices: intra-class and inter-class.\n",
    "\n",
    "### Between-class scatter matrix $S_W$\n",
    "The scatter matrix ** Between-class** $ S_W $ is calculated by the following equation:\n",
    "$$S_W = \\sum_{i=1}^{c}S_i$$\n",
    "\n",
    "Where,\n",
    "$$S_i = \\sum\\limits_{\\pmb x \\in D_i}^n (\\pmb x - \\pmb m_i)\\;(\\pmb x - \\pmb m_i)^T$$\n",
    "\n",
    "and $ m_i $ is the mean vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_W = np.zeros((4,4))\n",
    "for c,mv in zip(range(0,3), mean_vectors):\n",
    "    class_sc_mat = np.zeros((4,4))                  # scatter matrix for every class\n",
    "    for row in x[y == c]:\n",
    "        row, mv = row.reshape(4,1), mv.reshape(4,1) # make column vectors\n",
    "        class_sc_mat += (row-mv).dot((row-mv).T)\n",
    "    S_W += class_sc_mat                             # sum class scatter matrices\n",
    "print('between-class scatter matrix:\\n', S_W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could calculate the covariance matrices by adding a scaling factor $ \\frac {1} {N-1}$ to the between-class scatter matrix, as soon as our equation would become:\n",
    "\n",
    "$$\\Sigma_i = \\frac{1}{N_{i}-1} \\sum\\limits_{\\pmb x \\in D_i}^n (\\pmb x - \\pmb m_i)\\;(\\pmb x - \\pmb m_i)^T$$\n",
    "and\n",
    "\n",
    "$$S_W = \\sum\\limits_{i=1}^{c} (N_{i}-1) \\Sigma_i$$\n",
    "\n",
    "where $N_i$ is the sample size for the respective class (in our case: 50), and in this particular case, we could discard the term $(N_i-1)$ since all classes have the same sample size.\n",
    "\n",
    "However, the resulting autosomes would be the same (even eigenvectors, only the eigenvalues would be differently scaled by a constant factor).\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entre-classes scatter matrix $S_B$\n",
    "\n",
    "The scatter matrix **entre-classes*** $ S_B $ is calculated by the following equation:\n",
    "\n",
    "$$S_B =  \\sum\\limits_{i=1}^{c} N_{i} (\\pmb m_i - \\pmb m) (\\pmb m_i - \\pmb m)^T$$\n",
    "\n",
    "Where ** m ** is the general mean, and $m_i$ and $N_i$ are the sample mean and size of the respective classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_mean = np.mean(x, axis=0)\n",
    "\n",
    "S_B = np.zeros((4,4))\n",
    "for i, mean_vec in enumerate(mean_vectors):\n",
    "    n = x[y==i,:].shape[0]\n",
    "    mean_vec = mean_vec.reshape(4,1) # make column vector\n",
    "    overall_mean = overall_mean.reshape(4,1) # make column vector\n",
    "    S_B += n * (mean_vec - overall_mean).dot((mean_vec - overall_mean).T)\n",
    "    \n",
    "print('entre-classes scatter matrix:\\n', S_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## Step 3: Calculate the eigenvalues for the matrix $S_W^{-1} S_B$\n",
    "\n",
    "Now let us compute the eigenvalues for the matrix $S_W ^ - 1 S_B$ to obtain the linear discriminants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))\n",
    "\n",
    "for i in range(len(eig_vals)):\n",
    "    eigvec_sc = eig_vecs[:,i].reshape(4,1)\n",
    "    print('\\eigenvectors {0}: \\n{1}'.format(i+1, eigvec_sc.real))\n",
    "    print('eigenvalues {:}: {:.2e}'.format(i+1, eig_vals[i].real))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, both autovetore and eigenvalues give us information about distortion in a linear transformation: **eigenvectors are basically the direction of this distortion**, while **eigenvalues represent the scaling factor for eigenvectors that describe magnitude of the distortion **.\n",
    "\n",
    "If we are applying LDA for dimensionality reduction, eigenvectors are important since they will form the new axes of the new attribute subspace; the associated self-scores are particularly important because they tell us how \"informative\" the new axes are.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification of eigenvectors and eigenvalues\n",
    "We can check our eigenvectors and eigenvalues calculated by checking the following equation:\n",
    "$$Av = \\lambda v$$\n",
    "\n",
    "where,\n",
    "$$A = S_W^{-1}S_B$$\n",
    "$$v = eigenvectors$$\n",
    "$$\\lambda = eigenvalues$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(eig_vals)):\n",
    "    eigv = eig_vecs[:,i].reshape(4,1)\n",
    "    print(np.linalg.inv(S_W).dot(S_B).dot(eigv).T, (eig_vals[i]*eigv).T, sep='\\n', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## Step 4: Selection of linear discriminants\n",
    "### Sorting eigenvectors by eigenvalues in descending order\n",
    "We are not interested in just projecting the data into a subspace that increases the separation between classes, but also reduces the dimensionality of our attribute space. However, eigenvectors only define the directions of the new axes, since they are all unit vectors.\n",
    "\n",
    "Therefore, to decide which eigenvector (s) we will choose, we will look at the corresponding eigenvalues of each eigenvector. Generally speaking, let us look at the eigenvectors with the highest eigenvalues associated with them.\n",
    "\n",
    "The most common way to do this is to sort the eigenvalues in descending order and choose the top-k eigenvectors associated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of tuples (eigenvalue, eigenvector)\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Sort the pair in descending order\n",
    "eig_pairs.sort\n",
    "eig_pairs.reverse\n",
    "\n",
    "print('Eigenvalues in descending order:')\n",
    "for i in eig_pairs:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By analyzing the eigenvalues, we can see that 2 eigenvalues are close to zero. This happens because they do not carry any associated information. In fact, they should be exactly 0, but they are not due to imprecision of floating points. In LDA, the number of linear discriminants is at most $c-1$ where **c** is the number of classes, since the sparse matrix $ -s_B $ is the sum of the **c** matrices with rank 1 or less. Note that in the rare case of perfect colinearity (all points of the sample aligned in a straight line), the covariance matrix should have rank 1, which would result in only one eigenvector with an eigenvalue other than zero.\n",
    "\n",
    "Now, let's calculate the ** explained variance ** in terms of percentage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Variance explained')\n",
    "eigv_sum = sum(eig_vals)\n",
    "for i,j in enumerate(eig_pairs):\n",
    "    print('eigenvalue {0:}: {1:.2%}'.format(i+1, (j[0]/eigv_sum).real))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first autopare is by far the most representative, and we will not lose much information if we form a 1D space based on this autopare.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the *k* eigenvectors with the highest eigenvalues\n",
    "After sorting the autopares in descending order by eigenvalues, now it's time to construct our $k \\times d$-dimensional eigenvector matrix w ($4 \\times 2$: choosing the 2 most informative autopares) and thus reducing our initial 4-dimensional space in a 2-dimensional subspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.hstack((eig_pairs[0][1].reshape(4,1), eig_pairs[1][1].reshape(4,1)))\n",
    "print('Matrix W:', W.real, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## Step 5: Transforming the samples into the new subspace\n",
    "Finally, let's use our **W** matrix (4x2) to transform our samples into the new subspace through the equation:\n",
    "\n",
    "$$Y = X \\times W$$\n",
    "\n",
    "where,\n",
    "- **X**: matrix $n \\times d$ representing the *n* samples\n",
    "- **Y**: transformed matrix $n \\times k$ of the samples in the new subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x.dot(W)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_step_lda():\n",
    "    label_dict = {0: 'Setosa', 1: 'Versicolor', 2:'Virginica'}\n",
    "    \n",
    "    ax = plt.subplot(111)\n",
    "    for label,marker,color in zip(\n",
    "        range(0,3),('^', 's', 'o'),('blue', 'red', 'green')):\n",
    "\n",
    "        plt.scatter(x=X[:,0].real[y == label],\n",
    "                y=X[:,1].real[y == label],\n",
    "                marker=marker,\n",
    "                color=color,\n",
    "                alpha=0.5,\n",
    "                label=label_dict[label]\n",
    "                )\n",
    "\n",
    "    plt.xlabel('LD1')\n",
    "    plt.ylabel('LD2')\n",
    "\n",
    "    leg = plt.legend(loc='upper right', fancybox=True)\n",
    "    leg.get_frame().set_alpha(0.5)\n",
    "    plt.title('LDA: Iris projection onto the first 2 linear discriminants')\n",
    "\n",
    "    # hide axis ticks\n",
    "    plt.tick_params(axis=\"true\", which=\"true\", bottom=\"false\", top=\"false\",  \n",
    "            labelbottom=\"true\", left=\"false\", right=\"false\", labelleft=\"true\")\n",
    "\n",
    "    # remove axis spines\n",
    "    ax.spines[\"top\"].set_visible(False)  \n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"bottom\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_visible(False)    \n",
    "\n",
    "    plt.grid()\n",
    "    plt.tight_layout\n",
    "    plt.show()\n",
    "\n",
    "plot_step_lda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph above represents our new subspace that we built via LDA. We can see that our first linear discriminant \"LD1\" separates the classes from quite satisfactory form. However, the second discriminant, \"LD2\", does not add much useful information, as we saw when calculating the explained variance.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison between PCA and LDA\n",
    "To compare the subspace of attributes that we obtain by LDA, we will use the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\">PCA do scikit-learn</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "\n",
    "sklearn_pca = sklearnPCA(n_components=2)\n",
    "X_pca = sklearn_pca.fit_transform(x)\n",
    "\n",
    "def plot_pca():    \n",
    "    label_dict = {0: 'Setosa', 1: 'Versicolor', 2:'Virginica'}\n",
    "    ax = plt.subplot(111)\n",
    "\n",
    "    for label,marker,color in zip(\n",
    "        range(0,3),('^', 's', 'o'),('blue', 'red', 'green')):\n",
    "\n",
    "        plt.scatter(x=X_pca[:,0][y == label],\n",
    "                y=X_pca[:,1][y == label],\n",
    "                marker=marker,\n",
    "                color=color,\n",
    "                alpha=0.5,\n",
    "                label=label_dict[label]\n",
    "                )\n",
    "\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "\n",
    "    leg = plt.legend(loc='upper right', fancybox=True)\n",
    "    leg.get_frame().set_alpha(0.5)\n",
    "    plt.title('PCA: Iris projection onto the first 2 principal components')\n",
    "\n",
    "    # hide axis ticks\n",
    "    plt.tick_params(axis=\"true\", which=\"true\", bottom=\"flase\", top=\"flase\",  \n",
    "            labelbottom=\"true\", left=\"flase\", right=\"flase\", labelleft=\"true\")\n",
    "\n",
    "    # remove axis spines\n",
    "    ax.spines[\"top\"].set_visible(False)  \n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"bottom\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_visible(False)    \n",
    "\n",
    "    plt.tight_layout\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_pca()\n",
    "plot_step_lda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two plots above nicely confirm what we have discussed before: Where the PCA accounts for the most variance in the whole dataset, the LDA gives us the axes that account for the most variance between the individual classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# LDA via scikit-learn\n",
    "Now, after we have seen how an Linear Discriminant Analysis works using a step-by-step approach, there is also a more convenient way to achive the same via the LDA class implemented in the [scikit-learn](https://scikit-learn.org/stable/) machine learning library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "sklearn_lda = LDA(n_components=2)\n",
    "x_lda_sklearn = sklearn_lda.fit_transform(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scikit_lda(X, title):\n",
    "    label_dict = {0: 'Setosa', 1: 'Versicolor', 2:'Virginica'}\n",
    "    ax = plt.subplot(111)\n",
    "    for label,marker,color in zip(\n",
    "        range(0,3),('^', 's', 'o'),('blue', 'red', 'green')):\n",
    "\n",
    "        plt.scatter(x=X[:,0][y == label],\n",
    "                    y=X[:,1][y == label] * -1, # flip the figure\n",
    "                    marker=marker,\n",
    "                    color=color,\n",
    "                    alpha=0.5,\n",
    "                    label=label_dict[label])\n",
    "\n",
    "    plt.xlabel('LD1')\n",
    "    plt.ylabel('LD2')\n",
    "\n",
    "    leg = plt.legend(loc='upper right', fancybox=True)\n",
    "    leg.get_frame().set_alpha(0.5)\n",
    "    plt.title(title)\n",
    "\n",
    "    # hide axis ticks\n",
    "    plt.tick_params(axis=\"true\", which=\"true\", bottom=\"flase\", top=\"flase\",  \n",
    "            labelbottom=\"true\", left=\"flase\", right=\"flase\", labelleft=\"true\")\n",
    "\n",
    "    # remove axis spines\n",
    "    ax.spines[\"top\"].set_visible(False)  \n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"bottom\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_visible(False)    \n",
    "\n",
    "    plt.grid()\n",
    "    plt.tight_layout\n",
    "    plt.show()\n",
    "\n",
    "plot_step_lda()\n",
    "plot_scikit_lda(x_lda_sklearn, title='Default LDA via scikit-learn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two plots above nicely confirm what we have discussed before: Where the PCA accounts for the most variance in the whole dataset, the LDA gives us the axes that account for the most variance between the individual classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# Note on Standardization\n",
    "Attribute scaling (such as standardization) **does not** affect the overall results of an LDA and therefore may be optional. Obviously, the scatter matrices will be different depending on whether the attributes were normalized or not. Also, eigenvectors will be different as well. However, the important thing is that the eigenvalues will be exactly the same as the final projections -the only difference you'll notice is the scale of the axes. This can be shown mathematically (future works) and you can find in the references the practical demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "1. <a href=\"http://sebastianraschka.com/Articles/2014_python_lda.html\">http://sebastianraschka.com/Articles/2014_python_lda.html</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
